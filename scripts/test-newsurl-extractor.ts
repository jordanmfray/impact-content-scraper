#!/usr/bin/env tsx
import { extractArticleUrlsFromNewsPage } from '../src/lib/newsUrlExtractor';

async function testNewsUrlExtraction() {
  try {
    console.log('ğŸ§ª Testing simple HTTP + AI approach for news URL extraction...\n');

    // Test with Safe House Project press page
    const testUrl = 'https://www.safehouseproject.org/press/';
    const orgName = 'Safe House Project';

    console.log(`ğŸ“° Testing with: ${orgName}`);
    console.log(`ğŸ”— News URL: ${testUrl}\n`);

    const startTime = Date.now();
    const urls = await extractArticleUrlsFromNewsPage(testUrl, orgName);
    const endTime = Date.now();

    console.log(`\nâ±ï¸  Extraction completed in ${endTime - startTime}ms`);
    console.log(`ğŸ“Š Results:`);
    console.log(`   âœ… URLs found: ${urls.length}`);
    
    if (urls.length > 0) {
      console.log(`\nğŸ“‹ Extracted URLs:`);
      urls.forEach((url, index) => {
        console.log(`   ${index + 1}. ${url}`);
      });
    } else {
      console.log(`\nâŒ No URLs extracted`);
    }

    console.log(`\nğŸ¯ Next step would be to scrape these ${urls.length} URLs individually using the existing bulk-scrape API.`);
    console.log(`   This approach is:`);
    console.log(`   âœ… Simple - just HTTP + AI`);
    console.log(`   âœ… Reliable - no external API dependencies`);
    console.log(`   âœ… Cost-effective - uses your curated newsUrl`);
    console.log(`   âœ… Targeted - focuses on actual articles, not random Google results`);

  } catch (error) {
    console.error('âŒ Test failed:', error);
    process.exit(1);
  }
}

testNewsUrlExtraction();
